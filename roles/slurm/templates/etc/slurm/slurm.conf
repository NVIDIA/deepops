#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ slurm_cluster_name }}
{% if slurm_enable_ha %}
{% for node_name in groups['slurm-master'] %}
SlurmctldHost={{ node_name }}
{% endfor %}
StateSaveLocation={{ slurm_ha_state_save_location }}
{% else %}
SlurmctldHost={{ groups['slurm-master'][0] }}
StateSaveLocation=/var/spool/slurm/ctld
{% endif %}

SlurmUser={{ slurm_username }}
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
{% if slurm_include_pmix %}
MpiDefault=pmix
{% endif %}
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
PluginDir={{ slurm_install_prefix }}/lib/slurm
#FirstJobId=
ReturnToService={{ slurm_return_to_service }}
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
PropagateResourceLimitsExcept=MEMLOCK
{% if slurm_contain_ssh is defined %}
PrologFlags=Alloc,Serial,Contain
{% else %}
PrologFlags=Alloc,Serial
{% endif %}
{% if slurm_enable_prolog_epilog %}
Prolog={{ slurm_config_dir }}/prolog.sh
Epilog={{ slurm_config_dir }}/epilog.sh
{% endif %}
{% if slurm_health_check_program is defined %}
HealthCheckProgram={{ slurm_health_check_program }}
HealthCheckInterval=300
HealthCheckNodeState=IDLE
{% endif %}
MailProg=/usr/bin/s-nail
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
TaskPlugin=affinity,cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
#
AccountingStorageTRES=gres/gpu
#DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ groups["slurm-master"][0] }}
#AccountingStorageLoc=
#AccountingStorageEnforce=associations,limits,qos
AccountingStorageUser={{ slurm_db_username }}
AccountingStoragePass=/var/run/munge/munge.socket.2
#
# COMPUTE NODES
{% if slurm_manage_gpus %}
GresTypes=gpu
{% endif %}
{% for node_name in groups['slurm-node'] %}
{% set memory =  hostvars[node_name]["ansible_local"]["memory"] -%}
{% set cpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["cpu_topology"] -%}
{% set gpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["gpu_topology"] -%}
    NodeName={{ node_name }}{{ " " -}}
    {% if slurm_manage_gpus %} {%- if gpu_topology|count %} Gres=gpu:{{ gpu_topology|count }} {% endif -%} {% endif %}
    CPUs={{ cpu_topology.logical_cpus|int }}{{ " " -}}
    Sockets={{ cpu_topology.sockets|int }}{{ " " -}}
    CoresPerSocket={{ cpu_topology.cores_per_socket|int }}{{ " " -}}
    ThreadsPerCore={{ cpu_topology.threads_per_core }}{{ " " -}}
    Procs={{ cpu_topology.sockets|int * cpu_topology.cores_per_socket|int }}{{ " " -}}
    RealMemory={{ memory.total_mb|int }}{{ " " -}}
    State=UNKNOWN
{% endfor %}

# hardcoding the partitions and default memory per node
# TODO: automatically define the partitions by resource
# TODO: set DefMemPerCPU = TotalMemory / LogicalCPUs
{% set slurm_oversubscribe = (slurm_manage_gpus == true)|ternary('NO', 'EXCLUSIVE') %}
PartitionName=batch Nodes=ALL Default=YES DefMemPerCPU=0 State=UP OverSubscribe={{ slurm_oversubscribe }} {%- if slurm_max_job_timelimit is defined %} MaxTime={{ slurm_max_job_timelimit }} {%- else %} MaxTime=INFINITE {% endif -%} {%- if slurm_default_job_timelimit is defined %} DefaultTime={{ slurm_default_job_timelimit }} {% endif -%}
