epel_package: "https://dl.fedoraproject.org/pub/epel/epel-release-latest-{{ ansible_distribution_major_version }}.noarch.rpm"

slurm_build_dir: /opt/deepops/build/slurm
hwloc_build_dir: /opt/deepops/build/hwloc
pmix_build_dir: /opt/deepops/build/pmix

slurm_workflow_build: yes
slurm_version: 20.11.7
slurm_src_url: "https://download.schedmd.com/slurm/slurm-{{ slurm_version }}.tar.bz2"
slurm_build_make_clean: no
slurm_build_dir_cleanup: no
slurm_config_dir: /etc/slurm
slurm_sysconf_dir: /etc/sysconfig
slurm_install_prefix: /usr/local
slurm_configure:  './configure --prefix={{ slurm_install_prefix }} --disable-dependency-tracking --disable-debug --disable-x11 --enable-really-no-cray --enable-salloc-kill-cmd --with-hdf5=no --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm --without-rpath --with-pmix={{ pmix_install_prefix }} --with-hwloc={{ hwloc_install_prefix }}'
slurm_force_rebuild: no
slurm_contain_ssh: yes

slurm_cluster_name: deepops
slurm_username: slurm
slurm_password: ReplaceWithASecurePasswordInTheVault
slurm_user_uid: 22078
slurm_user_home: /local/slurm

slurm_allow_ssh_user: []

slurm_db_username: slurm
slurm_db_password: AlsoReplaceWithASecurePasswordInTheVault

slurm_enable_ha: false
slurm_ha_state_save_location: "/sw/slurm"

is_controller: no
is_compute: no

# Controls when a DOWN node will be returned to service.
# See "man slurm.conf" for more details.
slurm_return_to_service: 1
slurm_reboot_program: "/bin/systemctl reboot"
slurm_resume_timeout: 900

# Sets: GresTypes=gpu
# Sets: Gres=gpu:{{ gpu_topology|count }}
slurm_manage_gpus: true

# Configuration file templates to use for configuring Slurm.
# The default values are relative paths that will point to files within the
# templates/ directory of this role, but we provide variables so that they
# can be overrided.
slurm_conf_template: "etc/slurm/slurm.conf"
slurm_cgroup_conf_template: "etc/slurm/cgroup.conf"
slurm_gres_conf_template: "etc/slurm/gres.conf"
slurm_dbd_conf_template: "etc/slurm/slurmdbd.conf"

# Controls the ability of the partition to execute more than one job at a time on each resource (node, socket or core depending upon the value of SelectTypeParameters). If resources are to be over-subscribed, avoiding memory over-subscription is very important. SelectTypeParameters should be configured to treat memory as a consumable resource and the --mem option should be used for job allocations. Sharing of resources is typically useful only when using gang scheduling (PreemptMode=suspend,gang). Possible values for OverSubscribe are "EXCLUSIVE", "FORCE", "YES", and "NO". Note that a value of "YES" or "FORCE" can negatively impact performance for systems with many thousands of running jobs. The default value is "NO". For more information see the following web pages: 
# Sets partition OverSubscribe
# To avoid sharing nodes, set to "EXCLUSIVE"
slurm_oversubscribe: "NO"

# Maximum run time limit for jobs. Format is minutes, minutes:seconds, hours:minutes:seconds, days-hours, days-hours:minutes, days-hours:minutes:seconds or "UNLIMITED". Time resolution is one minute and second values are rounded up to the next minute. This limit does not apply to jobs executed by SlurmUser or user root.
# Sets: MaxTime={{ slurm_max_job_timelimit }}
# Default: INFINITE
# slurm_max_job_timelimit: INFINITE

# Run time limit used for jobs that don't specify a value. If not set then MaxTime will be used. Format is the same as for MaxTime.
# Sets: DefaultTime={{ slurm_default_job_timelimit }}
# Default: <not included>
# slurm_default_job_timelimit: 

# Maximum time to wait for Slurm compute nodes to reboot during playbook setup
slurm_node_reboot_timeout: 600

# Set up prolog and epilog
slurm_enable_prolog_epilog: true

# Useful to delete old files (e.g. from previous versions of deepops)
slurm_clear_old_prolog_epilog: false

# Fileglob patterns used to copy prolog and epilog files
prolog_fileglob:
  - "templates/etc/slurm/prolog.d/*"
epilog_fileglob:
  - "templates/etc/slurm/epilog.d/*"

# Disable clearing of shared memory when user login shell exits
slurm_fix_shm: true

slurm_include_hwloc: yes
hwloc_version: 2.2.0
hwloc_tag: "v{{ hwloc_version.split('.')[0] }}.{{ hwloc_version.split('.')[1] }}"
hwloc_src_url: "https://download.open-mpi.org/release/hwloc/{{ hwloc_tag }}/hwloc-{{ hwloc_version }}.tar.gz"
hwloc_install_prefix: /opt/deepops/hwloc
hwloc_configure: "./configure --prefix={{ hwloc_install_prefix }} --enable-static --disable-cairo --disable-libxml2 --enable-netloc --enable-plugins"
hwloc_force_rebuild: no

slurm_include_pmix: yes
pmix_version: 2.2.4
pmix_src_url: "https://github.com/openpmix/openpmix/releases/download/v{{ pmix_version }}/pmix-{{ pmix_version }}.tar.bz2"
pmix_install_prefix: /opt/deepops/pmix
pmix_configure: "./configure --prefix={{ pmix_install_prefix }} --with-hwloc={{ hwloc_install_prefix }}"
pmix_force_rebuild: no
