---
- name: gather os specific variables
  include_vars: "{{ item }}"
  with_first_found:
    - files:
      - "{{ ansible_distribution|lower }}.yml"
      - "{{ ansible_os_family|lower }}.yml"
      paths:
      - ../vars
      skip: true
  tags:
    - always

- name: add epel repo
  yum_repository:
    name: epel
    description: EPEL YUM repo
    baseurl: "{{ epel_baseurl }}"
    gpgkey: "{{ epel_gpgkey }}"
  when: ansible_os_family == "RedHat"

- name: configure alternative library path
  template:
    src: ld-slurm.conf.j2
    dest: /etc/ld.so.conf.d/slurm.conf
    mode: 0644
  register: updated_slurm_ld

- name: update ld cache
  command: ldconfig
  when: updated_slurm_ld

- include: munge.yml

- include: hwloc.yml
  tags: hwloc
  when: slurm_include_hwloc or slurm_hpl

- include: pmix.yml
  tags: pmix
  when: slurm_include_pmix or slurm_hpl

- include: build.yml
  tags: build

- include: openblas.yml
  tags: openblas
  when: slurm_include_openblas or slurm_hpl

- include: openmpi.yml
  tags: openmpi
  when: slurm_include_openmpi or slurm_hpl

- name: create slurm user home
  file:
   path: "{{ slurm_user_home }}"
   recurse: yes
   state: directory

- name: create slurm user
  user:
    name: slurm
    state: present
    system: yes
    home: "{{ slurm_user_home }}"
    uid: "{{ slurm_user_uid }}"

- include: controller.yml
  when: is_controller

- include: compute.yml
  when: is_compute

- include: shmfix.yml
  when: is_compute and slurm_fix_shm

# un-drain nodes that are down due to an unexpected reboot during install
# sudo scontrol update node=XXX state=idle
# where XXX are the nodes that have changed and are marked as *down*
- name: set nodes to idle
  command: "scontrol update node={{ item }} state=idle"
  register: undrain_nodes_result
  ignore_errors: yes
  with_items:
    - "{{ groups['slurm-node'] }}"
  environment:
    PATH: '{{ slurm_install_prefix }}/bin:{{ ansible_env.PATH }}'
  tags:
    - undrain
